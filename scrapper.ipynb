{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper para obtener información de búsqueda de google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import random\n",
    "import requests\n",
    "import socket\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión al navegador\n",
    "\n",
    "Este espacio se utilza de prueba para la extracción de etiquetas en el s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "start_url = 'https://google.com'\n",
    "driver.get(start_url)\n",
    "# Modificar el tamaño de la ventana\n",
    "#driver.execute_script(\"document.body.style.zoom='50%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de archivos csv para obtener nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>\"JUAN ANTONIO VEGA RODRIGUEZ\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>\"FRANCISCO JAVIER RODRIGUEZ RUIZ\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>\"CRISTIAN ADRIAN TOVAR TORRES\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>\"SANTIAGO EUSEBIO GONZALEZ\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>\"MA ISABEL BALDERAS RAMIREZ\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Nombre\n",
       "201      \"JUAN ANTONIO VEGA RODRIGUEZ\"\n",
       "202  \"FRANCISCO JAVIER RODRIGUEZ RUIZ\"\n",
       "203     \"CRISTIAN ADRIAN TOVAR TORRES\"\n",
       "204        \"SANTIAGO EUSEBIO GONZALEZ\"\n",
       "205       \"MA ISABEL BALDERAS RAMIREZ\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init=200\n",
    "final=1000\n",
    "\n",
    "df = pd.read_excel('Z:/Data/HSBC/Asignaciones/ASG_TDC_IA_ENERO_JUNIO.xlsx')\n",
    "df['Nombre'] = df.Nombre.apply(lambda X: '\"'+X.replace('/', ' ').replace('*', '').rstrip()+'\"')\n",
    "df_filtered = df['Nombre']\n",
    "df_filtered.drop_duplicates(inplace=True)\n",
    "df_input = pd.DataFrame(df_filtered).iloc[init:final]\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenado de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://http://162.223.94.164:80'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/800 [00:10<2:22:36, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://http://130.61.49.108:3128'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/800 [00:20<2:17:23, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://http://155.94.241.132:3128'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/800 [00:23<2:35:05, 11.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Configurar la solicitud con el proxy seleccionado\u001b[39;00m\n\u001b[0;32m     21\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(start_url, proxies\u001b[39m=\u001b[39mproxy)\n\u001b[1;32m---> 23\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     25\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(options\u001b[39m=\u001b[39mchrome_options)\n\u001b[0;32m     26\u001b[0m driver\u001b[39m.\u001b[39mget(start_url)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "data_ = []\n",
    "data = []\n",
    "nombres = []\n",
    "\n",
    "start_url = 'https://google.com'\n",
    "# Cargar la lista de proxies desde el archivo\n",
    "with open ('proxies.csv', 'r') as file :\n",
    "    proxies_list = file.read().splitlines()\n",
    "    \n",
    "\n",
    "\n",
    "for i in tqdm(df_input['Nombre']):\n",
    "    \n",
    "    # Opciones del controlador para google.com\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Elegir una dirección de proxy aleatoria de la lista para cada solicitud\n",
    "    proxy = {'http': f'http://{random.choice(proxies_list)}'}\n",
    "    print(proxy)\n",
    "    # Configurar la solicitud con el proxy seleccionado\n",
    "    response = requests.get(start_url, proxies=proxy)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(start_url)\n",
    "    \n",
    "    # Acción de búsqueda en el input \n",
    "    nombres.append(i)\n",
    "    wait = WebDriverWait(driver, 100000)\n",
    "    nombre = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'gLFyf')))\n",
    "    nombre.send_keys(i)\n",
    "    nombre.send_keys(Keys.ENTER)\n",
    "\n",
    "    # Esperar a que los resultados de búsqueda se carguen completamente\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Obtener el HTML de la página actual\n",
    "    html_link = driver.page_source\n",
    "    soup = BeautifulSoup(html_link, 'html.parser')\n",
    "\n",
    "    # Extracción de clases\n",
    "    div = soup.find_all('div', class_=\"kvH3mc BToiNc UK95Uc\")\n",
    "    cont = soup.find_all('div', class_=\"Z26q7c UK95Uc\")\n",
    "\n",
    "    # Primer bucle para extraer la clase que conforma el bloque que corresponde al URL desplegado\n",
    "    if div:\n",
    "        for tag in div:\n",
    "            href_tags = tag.find_all('a', href=True)\n",
    "            text = tag.text\n",
    "\n",
    "            for href_tag in href_tags:\n",
    "                href = href_tag['href']\n",
    "\n",
    "                data.append({'href': href, 'Text': text, 'Nombre': i})\n",
    "    else:\n",
    "        data.append({'href': None, 'Text': None, 'Nombre': i})\n",
    "\n",
    "    # Segundo bucle para extraer los resúmenes de las URL desplegadas\n",
    "    if cont:\n",
    "        for tag in cont:\n",
    "            text = ''\n",
    "\n",
    "            span_tags = tag.find_all('span')\n",
    "            if span_tags:\n",
    "                for span_tag in span_tags:\n",
    "                    if span_tag.get('class', []) == ['ITHCWe']:\n",
    "                        continue\n",
    "                    text += span_tag.text.strip() + ' '\n",
    "            else:\n",
    "                text = tag.text.strip()\n",
    "\n",
    "            data_.append({'Text': text.strip()})\n",
    "    else:\n",
    "        data_.append({'Text': None})\n",
    "\n",
    "    #google = driver.find_element(By.CLASS_NAME, 'logo Ib7Efc')\n",
    "    #google.click()\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Rellenar los datos faltantes en data y data_ para tener la misma longitud\n",
    "    max_length = max(len(data), len(data_))\n",
    "    data += [{'href': None, 'Text': None}] * (max_length - len(data))\n",
    "    data_ += [{'Text': None}] * (max_length - len(data_))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_2 = pd.DataFrame(data_)\n",
    "\n",
    "df_3 = pd.concat([df, df_2], axis=1)\n",
    "df_3.dropna(inplace=True)\n",
    "# Filtrado por tipo de página encontrada\n",
    "# df_3 = df_3[df_3['href'].str.contains('facebook|twitter|instagram|linkedin|misprofesores|.org|tiktok', case=False)]\n",
    "# Se quitan enlaces para traducir páginas\n",
    "df_3 = df_3[~df_3['href'].str.contains('translate.google')]\n",
    "\n",
    "\n",
    "# Se guarda el df en un csv\n",
    "df_3.to_csv('Z:/Data/HSBC/SCRAPPING/200_1000_.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
